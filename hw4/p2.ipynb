{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to run code:  \n",
    "- Place this notebook in the same folder with all npy files used\n",
    "- Run the following code cells one by one except the appendix part (which is used for debugging)\n",
    "- Get the result in test_result.csv\n",
    "  \n",
    "Model architecture:  \n",
    "- The model consists of encoder, decoder and attention part\n",
    "- Encoder has a bidirectional LSTM layer and three pBLSTM layer, each halving the sequence length, and two linear layer for calculating keys and values\n",
    "- Attention is calculated in decoder, using key, value, and the hidden state of decoder as query\n",
    "- Decoder has an embedding layer, two LSTM layer implemented by LSTMcell, and a linear layer to predict letter\n",
    "  \n",
    "Hyperparameters:\n",
    "- Input data uses data augmentation, specifically frequency masking with param=5 and time masking with param=30\n",
    "- Encoder has a hidden size of 256, and decoder 512\n",
    "- The size of key and value are both 128\n",
    "- Decoder uses weight tying between embedding layer and prediction layer\n",
    "- Use Adam with initial learning rate 1e-3 and weight decay 5e-6\n",
    "- Use a scheduler to half the learning rate if the validation distance doesn't improve 0.2% in 3 epochs\n",
    "- Use cross entropy loss\n",
    "\n",
    "Description:  \n",
    "First a mapping between letters and their indices is formed by retrieving all letters in training data, and is represented by a list index2letter and a dictionary letter2index. Then in each batch the dataloader randomly sample 32 pieces of speech and their text labels, then convert labels into indices in alphabet letter by letter.\n",
    "After getting a batch, the model first calculate its representation in encoder by pyramid LSTM, then get keys and values in every time step. Then in decoder it decode the representation step by step, and calculate attention using all keys and values and the hidden state in each step. Besides, it uses lengths of training data to mask attentions, and uses lengths of labels to mask losses, to get rid of padded ones. Loss is added up in each sequence, and the average loss of a batch is used for back propagation.\n",
    "The current state of the model is saved once its validation L distance breaks the record. After the training process complete, the last model state saved will be retrieved and used for the speech to text task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9ZDNDfHUOUF"
   },
   "source": [
    "# Set up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 203609,
     "status": "ok",
     "timestamp": 1620577320615,
     "user": {
      "displayName": "Ruiyang Jin",
      "photoUrl": "",
      "userId": "11822135231472973432"
     },
     "user_tz": -480
    },
    "id": "oEPq0A46TH6p"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from Levenshtein import distance\n",
    "import seaborn as sns\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 202327,
     "status": "ok",
     "timestamp": 1620577320616,
     "user": {
      "displayName": "Ruiyang Jin",
      "photoUrl": "",
      "userId": "11822135231472973432"
     },
     "user_tz": -480
    },
    "id": "XbqI2bMgTbbh",
    "outputId": "4fb963c7-c9fc-4fc0-df1d-8e8068a7795b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda = True with num_workers = 4\n",
      "Cuda = True with num_workers = 4\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "num_workers = 4 if cuda else 0\n",
    "print(\"Cuda = \"+str(cuda)+\" with num_workers = \"+str(num_workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AdPXOr-UUcR"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 219841,
     "status": "ok",
     "timestamp": 1620577339448,
     "user": {
      "displayName": "Ruiyang Jin",
      "photoUrl": "",
      "userId": "11822135231472973432"
     },
     "user_tz": -480
    },
    "id": "ukkjj5HWZVmv"
   },
   "outputs": [],
   "source": [
    "train_X = np.load(\"train.npy\", allow_pickle=True)\n",
    "val_X = np.load(\"dev.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 551,
     "status": "ok",
     "timestamp": 1620577351289,
     "user": {
      "displayName": "Ruiyang Jin",
      "photoUrl": "",
      "userId": "11822135231472973432"
     },
     "user_tz": -480
    },
    "id": "ZFmcvE1pkDNi"
   },
   "outputs": [],
   "source": [
    "# preprocess transcripts, transform it to indices and build dictionary\n",
    "\n",
    "# retrieve all characters from train and dev transcripts\n",
    "def create_dictionaries(raw_transcripts):\n",
    "    '''\n",
    "    Create dictionaries for letter2index and index2letter transformations\n",
    "    Return a list of all possible letters for index2letter,\n",
    "    and a dictionary for letter2index\n",
    "    '''\n",
    "    letter_list = [\"<sos>\", \"<eos>\", \" \"]   # include start/end of sentence and space\n",
    "    for transcript in raw_transcripts:\n",
    "        for word in transcript:\n",
    "            word = str(word, \"utf-8\")\n",
    "            for character in word:\n",
    "                if character not in letter_list:\n",
    "                    letter_list.append(character)\n",
    "    letter_list.remove('\\x00')\n",
    "    dictionary = {}\n",
    "    for i, character in enumerate(letter_list):\n",
    "        dictionary[character] = i\n",
    "    return letter_list, dictionary\n",
    "\n",
    "\n",
    "def transform_letter_to_index(raw_transcripts, letter2index):\n",
    "    '''\n",
    "    Transforms text input to numerical input by converting each letter \n",
    "    to its corresponding index from letter_list\n",
    "\n",
    "    Args:\n",
    "        raw_transcripts: Raw text transcripts with the shape of (N, )\n",
    "    \n",
    "    Return:\n",
    "        transcripts: Converted index-format transcripts. This would be a list with a length of N\n",
    "          each element would be a numpy-array containing indices of a sentence\n",
    "    '''\n",
    "    transcripts = []  \n",
    "    for transcript in raw_transcripts:\n",
    "        sentence = \" \".join([str(word, \"utf-8\") for word in transcript])\n",
    "        # sentence = transcript[0]  # for toy data\n",
    "        transcript_indices = np.array([letter2index[\"<sos>\"]] + [letter2index[c] for c in sentence] + [letter2index[\"<eos>\"]])    # add sos and eos for decoder\n",
    "        transcripts.append(transcript_indices)\n",
    "    return transcripts\n",
    "\n",
    "\n",
    "# transform indices back to letters for submission\n",
    "# enter a 1D tensor consisting of indices of characters in a sequence, return the sequence string\n",
    "def transform_index_to_letter(sentence_indices, index2letter):\n",
    "    sentence = \"\"\n",
    "    for i in sentence_indices:\n",
    "        c = index2letter[i]\n",
    "        if c == \"<sos>\":\n",
    "            continue\n",
    "        if c == \"<eos>\":\n",
    "            return sentence\n",
    "        sentence += c\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3499,
     "status": "ok",
     "timestamp": 1620577357200,
     "user": {
      "displayName": "Ruiyang Jin",
      "photoUrl": "",
      "userId": "11822135231472973432"
     },
     "user_tz": -480
    },
    "id": "amWdvSwZupYX"
   },
   "outputs": [],
   "source": [
    "train_Y_raw = np.load(\"train_transcripts.npy\", allow_pickle=True)\n",
    "val_Y_raw = np.load(\"dev_transcripts.npy\", allow_pickle=True)\n",
    "index2letter, letter2index = create_dictionaries([train_Y_raw, val_Y_raw])\n",
    "train_Y = transform_letter_to_index(train_Y_raw, letter2index)\n",
    "val_Y = transform_letter_to_index(val_Y_raw, letter2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 544,
     "status": "ok",
     "timestamp": 1620577378571,
     "user": {
      "displayName": "Ruiyang Jin",
      "photoUrl": "",
      "userId": "11822135231472973432"
     },
     "user_tz": -480
    },
    "id": "sFrnrZK9eTeY"
   },
   "outputs": [],
   "source": [
    "# define dataset\n",
    "train_transform = nn.Sequential(\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=5),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=30)\n",
    ")\n",
    "\n",
    "class LASDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, Y, letter2index, train=False):\n",
    "        # directly assign the original data instead of copying to save memory\n",
    "        # Y could be none in case of test\n",
    "        self.length = len(X)\n",
    "        self.X = X    # (number of samples, indefinite number of time steps, feature_length), ndarray of objects(ndarrays)\n",
    "        self.Y = Y    # (number of samples, indefinite number of time steps), ndarray of objects(ndarrays)\n",
    "        self.letter2index = letter2index\n",
    "        self.train = train\n",
    "  \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "  \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.X[index]\n",
    "        # train/validation\n",
    "        if self.Y is not None:\n",
    "            y = self.Y[index]\n",
    "            return x, y\n",
    "        # test\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    # used to retrieve lengths of sequences and pad sequences\n",
    "    # then convert result to tensor\n",
    "    # batch_first=True for both input and label, and all across the training\n",
    "    def collate_fn(self, data):\n",
    "        inputs = []\n",
    "        labels = []\n",
    "        input_lengths = torch.zeros(len(data)).long()   # used to add mask in attention\n",
    "        label_lengths = torch.zeros(len(data)).long()   # used to add mask in loss\n",
    "        # train/val\n",
    "        if self.Y is not None:\n",
    "            for i, (x, y) in enumerate(data):\n",
    "                if self.train:\n",
    "                    inputs.append(train_transform(torch.tensor(x).permute(1,0)).permute(1,0))  # data augmentation\n",
    "                else:\n",
    "                    inputs.append(torch.tensor(x))  # data augmentation\n",
    "                labels.append(torch.tensor(y))\n",
    "                input_lengths[i] = len(x)\n",
    "                label_lengths[i] = len(y)\n",
    "            padded_inputs = rnn.pad_sequence(inputs, batch_first=True)  # tensor of batch_size*longest_input_length*feature_length\n",
    "            '''\n",
    "            # data augmentation\n",
    "            if self.length > 10000:\n",
    "                train_transforms = nn.Sequential(\n",
    "                    torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
    "                    torchaudio.transforms.TimeMasking(time_mask_param=35)\n",
    "                )\n",
    "                padded_inputs = train_transforms(padded_inputs.permute(1, 2, 0))\n",
    "                padded_inputs = padded_inputs.permute(2, 0, 1)\n",
    "            '''\n",
    "            padded_labels = rnn.pad_sequence(labels, batch_first=True, padding_value=self.letter2index[\"<eos>\"])  # tensor of batch_size*longest_label_length, pad with <eos> index instead of 0\n",
    "            return padded_inputs.float(), padded_labels.long(), input_lengths, label_lengths\n",
    "        # test\n",
    "        else:\n",
    "            for i, x in enumerate(data):\n",
    "                inputs.append(torch.tensor(x))\n",
    "                input_lengths[i] = len(x)\n",
    "            padded_inputs = rnn.pad_sequence(inputs, batch_first=True)  # tensor of batch_size*longest_input_length*feature_length\n",
    "            return padded_inputs.float(), input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1620577387618,
     "user": {
      "displayName": "Ruiyang Jin",
      "photoUrl": "",
      "userId": "11822135231472973432"
     },
     "user_tz": -480
    },
    "id": "YkArRQ0FTcqL"
   },
   "outputs": [],
   "source": [
    "# load training data\n",
    "# set drop_last=True for counting samples in calculating average accuracy\n",
    "batch_size = 32\n",
    "train_dataset = LASDataset(train_X, train_Y, letter2index, train=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=num_workers, collate_fn=train_dataset.collate_fn)\n",
    "val_dataset = LASDataset(val_X, val_Y, letter2index)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True, num_workers=num_workers, collate_fn=val_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b42dLdzdUg8Q"
   },
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 524,
     "status": "ok",
     "timestamp": 1620577391521,
     "user": {
      "displayName": "Ruiyang Jin",
      "photoUrl": "",
      "userId": "11822135231472973432"
     },
     "user_tz": -480
    },
    "id": "EfpIMUDzCvT3"
   },
   "outputs": [],
   "source": [
    "class pBLSTM(nn.Module):\n",
    "    '''\n",
    "    Pyramidal BiLSTM\n",
    "    Read paper and understand the concepts and then write your implementation here.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(pBLSTM, self).__init__()\n",
    "        self.blstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "\n",
    "    # input: N * time_steps * input_dim (padded)\n",
    "    # output: N * time_steps * hiddex_dim\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, input_dim = x.shape\n",
    "        # truncate odd-length sequence\n",
    "        if seq_len % 2:\n",
    "            x = x[:,:-1,:]\n",
    "        x = x.reshape(batch_size, seq_len//2, input_dim*2)   # use concatenation for adjacent feature vectors\n",
    "        out, _ = self.blstm(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 558,
     "status": "ok",
     "timestamp": 1620577397749,
     "user": {
      "displayName": "Ruiyang Jin",
      "photoUrl": "",
      "userId": "11822135231472973432"
     },
     "user_tz": -480
    },
    "id": "-F9zAQR95P55"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Encoder takes the utterances as inputs and returns the key, value and unpacked_x_len.\n",
    "    Key and value are linear projections of the output from pBLSTM network for the laster.\n",
    "    '''\n",
    "    def __init__(self, input_dim, encoder_hidden_dim, key_value_size=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        # The first LSTM at the very bottom\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=encoder_hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "\n",
    "        # TODO: Define the blocks of pBLSTMs\n",
    "        # take 2 hidden vectors of length hidden_dim*2, output one vector of the same length\n",
    "        self.pblstm1 = pBLSTM(encoder_hidden_dim * 4, encoder_hidden_dim)\n",
    "        self.pblstm2 = pBLSTM(encoder_hidden_dim * 4, encoder_hidden_dim)\n",
    "        self.pblstm3 = pBLSTM(encoder_hidden_dim * 4, encoder_hidden_dim)\n",
    "         \n",
    "        # The linear transformation for producing Key and Value for attention\n",
    "        # Since you are using bidirectional LSTM, be careful about the size of hidden dimension\n",
    "        # use the same length for key and value\n",
    "        self.key_network = nn.Linear(encoder_hidden_dim * 2, key_value_size)\n",
    "        self.value_network = nn.Linear(encoder_hidden_dim * 2, key_value_size)\n",
    "\n",
    "\n",
    "    # input: N * seq_len * feature_length (padded, seq_len is the longest sequence length)\n",
    "    # output: N * seq_len * (hidden_dim*2)\n",
    "    def forward(self, x, x_len):\n",
    "        # Pass through the first LSTM at the very bottom\n",
    "        packed_sequence = rnn.pack_padded_sequence(x, x_len.cpu(), enforce_sorted=False, batch_first=True)\n",
    "        packed_out, _ = self.lstm(packed_sequence)\n",
    "        output, out_lengths = rnn.pad_packed_sequence(packed_out, batch_first=True)   # N * seq_len * (hidden*2), with N lengths\n",
    "\n",
    "        # TODO: Pass through the pBLSTM blocks\n",
    "        # can't pass packedsequence into pBLSTM, need to use unpacked sequences\n",
    "        # update sequence lengths simultaneously\n",
    "        # output is now N * (seq_len//8) * (hidden*2)\n",
    "        output = self.pblstm1(output)\n",
    "        out_lengths = out_lengths // 2\n",
    "        output = self.pblstm2(output)\n",
    "        out_lengths = out_lengths // 2\n",
    "        output = self.pblstm3(output)\n",
    "        out_lengths = out_lengths // 2\n",
    "        \n",
    "        # Unpack the sequence and get the Key and Value for attention\n",
    "        # shape is N * (seq_len//8) * key_value_size\n",
    "        key = self.key_network(output)\n",
    "        value = self.value_network(output)\n",
    "\n",
    "        # return key, value, unpacked_x_len\n",
    "        # out_lengths is used for masking in calculating attention\n",
    "        return key, value, out_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 592,
     "status": "ok",
     "timestamp": 1620577402235,
     "user": {
      "displayName": "Ruiyang Jin",
      "photoUrl": "",
      "userId": "11822135231472973432"
     },
     "user_tz": -480
    },
    "id": "pqu-MUM8TjUO"
   },
   "outputs": [],
   "source": [
    "def plot_attention(attention):\n",
    "    plt.clf()\n",
    "    sns.heatmap(attention, cmap='GnBu')\n",
    "    plt.show()\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    '''\n",
    "    Attention is calculated using key, value and query from Encoder and decoder.\n",
    "    Below are the set of operations you need to perform for computing attention:\n",
    "        energy = bmm(key, query)\n",
    "        attention = softmax(energy)\n",
    "        context = bmm(attention, value)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    # refer recitation 8\n",
    "    # key/value: N * in_seq_len * key_value_size, the whole input key/value sequence\n",
    "    # query: N * key_value_size, query of one time step\n",
    "    def forward(self, query, key, value, mask):\n",
    "        energy = torch.bmm(key, query.unsqueeze(2)).squeeze(2)  # element-wise matrix multiplication in a batch, get energy shape N * in_seq_len\n",
    "        energy.masked_fill_(mask, -1e9)               # use a mask of shape (N, in_seq_len), get rid of invalid values\n",
    "        attention = F.softmax(energy, dim=1)            # (N, in_seq_len)\n",
    "        out = torch.bmm(attention.unsqueeze(1), value).squeeze(1) # Compute attention-weighted sum of context vectors of shape (N, key_value_size)\n",
    "        # attention vectors are returned for visualization\n",
    "        return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 855,
     "status": "ok",
     "timestamp": 1620577404872,
     "user": {
      "displayName": "Ruiyang Jin",
      "photoUrl": "",
      "userId": "11822135231472973432"
     },
     "user_tz": -480
    },
    "id": "zcTC4cK95TYT"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    As mentioned in a previous recitation, each forward call of decoder deals with just one time step.\n",
    "    Thus we use LSTMCell instead of LSTM here.\n",
    "    The output from the second LSTMCell can be used as query for calculating attention.\n",
    "    In place of value that we get from the attention, this can be replace by context we get from the attention.\n",
    "    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance.\n",
    "    '''\n",
    "    def __init__(self, vocab_size, decoder_hidden_dim, embed_dim, key_value_size=128, max_out_len=600):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=letter2index['<eos>'])\n",
    "        self.lstm1 = nn.LSTMCell(input_size=embed_dim + key_value_size, hidden_size=decoder_hidden_dim)\n",
    "        self.lstm2 = nn.LSTMCell(input_size=decoder_hidden_dim, hidden_size=key_value_size)\n",
    "        # in this implementation, the size of (key = query) = (value = context) = decoder_hidden\n",
    "      \n",
    "        self.attention = Attention()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.character_prob = nn.Linear(2 * key_value_size, vocab_size)   # input_dim is key_value_size(context) + hidden_size(hidden_state)\n",
    "        self.character_prob.weight = self.embedding.weight\n",
    "        self.key_value_size = key_value_size\n",
    "\n",
    "        self.max_out_len = max_out_len      # default maximum output sequence length\n",
    "\n",
    "    def forward(self, key, value, encoder_len, y=None, mode='train', teacher_forcing=0.9):\n",
    "        '''\n",
    "        Args:\n",
    "            key :(B, T, key_value_size) - Output of the Encoder Key projection layer\n",
    "            value: (B, T, key_value_size) - Output of the Encoder Value projection layer\n",
    "            y: (B, text_len) - Batch input of text label with text_length, used in teacher forcing\n",
    "            mode: Train or eval mode\n",
    "        Return:\n",
    "            predictions: the character perdiction probability\n",
    "        '''\n",
    "\n",
    "        batch_size, key_seq_max_len, key_value_size = key.shape\n",
    "\n",
    "        # get maximum length of results, or set to default value in testing, then just generate sequences of that length\n",
    "        # get teacher forcing ground truths in training\n",
    "        if mode == 'train':\n",
    "            max_len =  y.shape[1]\n",
    "            label_embeddings = self.embedding(y)\n",
    "        else:\n",
    "            max_len = self.max_out_len\n",
    "\n",
    "        # TODO: Create the attention mask here (outside the for loop rather than inside) to avoid repetition\n",
    "        mask = torch.arange(key.size(1)).unsqueeze(0) >= encoder_len.unsqueeze(1)   # Make use of broadcasting: (1, seq_len), (batch_size, 1) -> (batch_size, seq_len)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        # output of every time step\n",
    "        predictions = []\n",
    "        prediction = torch.zeros(batch_size, 1).to(device)  # initialize to text output <sos> instead of logits in every batch for first input, compatible with the calculation of char_embed for logits below\n",
    "        hidden_states = [None, None]  # initial hidden states of every layer\n",
    "        \n",
    "        # TODO: Initialize the context. Be careful here\n",
    "        # initialize context to 0 since the first timestep doesn't have context\n",
    "        context = torch.zeros(batch_size, key_value_size).to(device)\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            # get one-time-step embedding in a batch of shape (batch_size, embed_size)\n",
    "            if mode == 'train':\n",
    "                if np.random.random() > teacher_forcing:\n",
    "                    if i > 0:\n",
    "                        prediction = F.gumbel_softmax(prediction) # Gumbel noise\n",
    "                    char_embed = self.embedding(prediction.argmax(dim=-1))\n",
    "                # Teacher Forcing\n",
    "                else:\n",
    "                    char_embed = label_embeddings[:,i]\n",
    "            else:\n",
    "                char_embed = self.embedding(prediction.argmax(dim=-1))\n",
    "\n",
    "            y_context = torch.cat([char_embed, context], dim=1)   # (batch_size, embed_size+key_value_size)\n",
    "            hidden_states[0] = self.lstm1(y_context, hidden_states[0])\n",
    "            lstm1_hidden = hidden_states[0][0]\n",
    "            hidden_states[1] = self.lstm2(lstm1_hidden, hidden_states[1])\n",
    "            output = hidden_states[1][0]    # (batch_size, hidden_size)\n",
    "            \n",
    "            # TODO: Compute attention from the output of the second LSTM Cell\n",
    "            context, attention_score = self.attention(output, key, value, mask)  # directly use hidden state as query\n",
    "            \n",
    "            output_context = torch.cat([output, context], dim=1)\n",
    "            prediction = self.character_prob(output_context)    # (batch_size, vocab_size)\n",
    "            predictions.append(prediction.unsqueeze(1))\n",
    "        return torch.cat(predictions, dim=1)    # (batch_size, max_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1620577410418,
     "user": {
      "displayName": "Ruiyang Jin",
      "photoUrl": "",
      "userId": "11822135231472973432"
     },
     "user_tz": -480
    },
    "id": "d35FEZhz5Uhx"
   },
   "outputs": [],
   "source": [
    "class LASmodel(nn.Module):\n",
    "    '''\n",
    "    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder.\n",
    "    This is simply a wrapper \"model\" for your encoder and decoder.\n",
    "    '''\n",
    "    def __init__(self, input_dim, vocab_size, encoder_hidden_dim, decoder_hidden_dim, embed_dim, key_value_size=128):\n",
    "        super(LASmodel,self).__init__()\n",
    "        self.encoder = Encoder(input_dim, encoder_hidden_dim, key_value_size=key_value_size)\n",
    "        self.decoder = Decoder(vocab_size, decoder_hidden_dim, embed_dim, key_value_size=key_value_size)\n",
    "\n",
    "    def forward(self, x, x_len, y=None, mode='train', teacher_force=0.9):\n",
    "        key, value, encoder_len = self.encoder(x, x_len)\n",
    "        predictions = self.decoder(key, value, encoder_len, y=y, mode=mode, teacher_forcing=teacher_force)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KI4wtFlyUmmm"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 651,
     "status": "ok",
     "timestamp": 1620577416486,
     "user": {
      "displayName": "Ruiyang Jin",
      "photoUrl": "",
      "userId": "11822135231472973432"
     },
     "user_tz": -480
    },
    "id": "wNdRSaUDTiQs"
   },
   "outputs": [],
   "source": [
    "# train one epoch, return the average training loss\n",
    "def train_epoch(model, train_loader, criterion, optimizer, teacher_force_rate):\n",
    "    training_loss = 0\n",
    "    model.train()\n",
    "    # 0) Iterate through your data loader\n",
    "    for batch_num, (padded_inputs, padded_labels, input_lengths, label_lengths) in tqdm(enumerate(train_loader)):\n",
    "        # 1) Set the inputs to the device.\n",
    "        padded_inputs = padded_inputs.to(device)\n",
    "        padded_labels = padded_labels.to(device)\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        label_lengths = label_lengths.to(device)\n",
    "        # 2) Pass your inputs, and length of speech into the model.\n",
    "        # the input to the decoder should be <sos>+sequence and output sequence+<eos>, while padded_labels=<sos>+sequence+<eos>\n",
    "        # so remove the last element of each label sequence to fix shape\n",
    "        predictions = model(padded_inputs, input_lengths, padded_labels[:,:-1], mode=\"train\", teacher_force=teacher_force_rate)\n",
    "        # 3) Generate a mask based on the lengths of the text\n",
    "        #    Ensure the mask is on the device and is the correct shape.\n",
    "        # use length-1 to mask out predictions out of <eos>\n",
    "        mask_loss = torch.arange(padded_labels.size(1)-1).unsqueeze(0).to(device) >= (label_lengths-1).unsqueeze(1)\n",
    "        # 4. Calculate the loss and mask it to remove the padding part\n",
    "        batch_size, max_len, vocab_size = predictions.shape\n",
    "        loss = criterion(predictions.reshape(-1, vocab_size), padded_labels[:,1:].reshape(-1)) # remove <sos> in labels to calculate loss\n",
    "        loss.masked_fill_(mask_loss.reshape(-1).to(device), 0)\n",
    "        loss = loss.sum() / batch_size   # add up losses of all time steps, then divide by batch size\n",
    "        training_loss += loss.item()\n",
    "        # 5. Backward on the masked loss\n",
    "        loss.backward()\n",
    "        # 6. Optional: Use torch.nn.utils.clip_grad_norm(model.parameters(), 2) to clip the gradient\n",
    "        # 7. Take a step with your optimizer\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    training_loss /= len(train_loader)\n",
    "    # 8. print the statistic (loss, edit distance and etc.) for analysis\n",
    "    return training_loss\n",
    "        \n",
    "\n",
    "# validation of classification task, return the average loss and LD\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    val_loss = 0\n",
    "    val_LD = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (padded_inputs, padded_labels, input_lengths, label_lengths) in enumerate(val_loader):\n",
    "            padded_inputs = padded_inputs.to(device)\n",
    "            padded_labels = padded_labels.to(device)\n",
    "            input_lengths = input_lengths.to(device)\n",
    "            label_lengths = label_lengths.to(device)\n",
    "\n",
    "            predictions = model(padded_inputs, input_lengths, padded_labels[:,:-1], mode=\"test\")\n",
    "            # calculate masked loss\n",
    "            # prediction will be of shape (batch_size, 600, vocab_size) since it's in test mode, need to truncate\n",
    "            predictions = predictions[:,:padded_labels.shape[1]-1]\n",
    "            mask_loss = torch.arange(padded_labels.size(1)-1).unsqueeze(0).to(device) >= (label_lengths-1).unsqueeze(1)\n",
    "            batch_size, max_len, vocab_size = predictions.shape\n",
    "            # remove <sos> in labels to calculate loss\n",
    "            loss = criterion(predictions.reshape(-1, vocab_size), padded_labels[:,1:].reshape(-1))\n",
    "            loss.masked_fill_(mask_loss.reshape(-1).to(device), 0)\n",
    "            loss = loss.sum() / batch_size   # add up losses of all time steps, then divide by batch size\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # simple greedy search decoding\n",
    "            predicted_indices = torch.argmax(predictions, dim=2)\n",
    "            # convert result back to text, and compute LD\n",
    "            batch_LD = 0\n",
    "            for i in range(batch_size):\n",
    "                letter_seq = transform_index_to_letter(predicted_indices[i], index2letter)\n",
    "                letter_label_seq = transform_index_to_letter(padded_labels[i][1:], index2letter)\n",
    "                batch_LD += distance(letter_seq, letter_label_seq)\n",
    "            batch_LD /= batch_size\n",
    "            val_LD += batch_LD\n",
    "    val_loss /= len(val_loader)\n",
    "    val_LD /= len(val_loader)\n",
    "    return val_loss, val_LD\n",
    "\n",
    "def test(model, test_loader):\n",
    "    final_pred = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for padded_inputs, input_lengths in test_loader:\n",
    "            padded_inputs = padded_inputs.to(device)\n",
    "            input_lengths = input_lengths.to(device)\n",
    "            predictions = model(padded_inputs, input_lengths, mode=\"test\")\n",
    "            # simple greedy search decoding\n",
    "            predicted_indices = torch.argmax(predictions, dim=2)\n",
    "            # convert result back to text\n",
    "            for i in range(len(padded_inputs)):\n",
    "                final_pred.append(transform_index_to_letter(predicted_indices[i], index2letter))\n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1343125,
     "status": "ok",
     "timestamp": 1620578901513,
     "user": {
      "displayName": "Ruiyang Jin",
      "photoUrl": "",
      "userId": "11822135231472973432"
     },
     "user_tz": -480
    },
    "id": "K5Hp_-V8hadn",
    "outputId": "18c30fd9-bfaf-4fab-d5bd-6e56de31077f"
   },
   "outputs": [],
   "source": [
    "# create model and other stuff\n",
    "feature_length = 40\n",
    "learningRate = 1e-3\n",
    "weightDecay = 5e-6\n",
    "vocab_size = len(index2letter)\n",
    "listener_hidden_size = 256\n",
    "speller_hidden_size = 512\n",
    "key_value_size = 128\n",
    "embed_size = 256\n",
    "model = LASmodel(feature_length, vocab_size, listener_hidden_size, speller_hidden_size, embed_size, key_value_size)\n",
    "model.to(device)\n",
    "model_name = \"LAS_DA_gumbel\"\n",
    "os.makedirs(f\"model/{model_name}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = optim.Adam(model.parameters(), lr=learningRate, weight_decay=weightDecay)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, gamma=0.5, step_size=5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3, threshold=0.002)\n",
    "'''\n",
    "checkpoint = torch.load(\"best_checkpoint\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "# checkpoint[\"optimizer_state_dict\"][\"param_groups\"][0][\"lr\"] = 5e-4\n",
    "criterion = checkpoint[\"loss\"]\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "trained_epoch = checkpoint[\"epoch\"] + 41\n",
    "# scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3, threshold=0.1)\n",
    "'''\n",
    "# train model\n",
    "epoch_num = 60\n",
    "best_val_LD = np.inf\n",
    "for epoch in range(epoch_num):\n",
    "    teacher_force_rate = 0.9 - 0.05 * (epoch//5)\n",
    "    training_loss = train_epoch(model, train_dataloader, criterion, optimizer, teacher_force_rate)\n",
    "    val_loss, val_LD = evaluate(model, val_dataloader, criterion)\n",
    "    scheduler.step(val_LD)\n",
    "    if(val_LD < best_val_LD):\n",
    "        best_val_LD = val_LD\n",
    "        torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": criterion,\n",
    "                \"scheduler_state_dict\": scheduler.state_dict()\n",
    "            }, f\"model/{model_name}/best_checkpoint\")\n",
    "    if(epoch % 10 == 9):\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": criterion,\n",
    "            \"scheduler_state_dict\": scheduler.state_dict()\n",
    "        }, f\"model/{model_name}/checkpoint_{epoch}\")\n",
    "    with open(f\"model/{model_name}/training_logs.txt\", 'a') as logfile:\n",
    "        logfile.write(f\"Epoch: {epoch}, training loss: {training_loss}, validation loss: {val_loss}, validation Levenshtein Distance: {val_LD}, learning rate: {scheduler._last_lr}\\n\")\n",
    "    print(f\"Epoch: {epoch}, training loss: {training_loss}, validation loss: {val_loss}, validation Levenshtein Distance: {val_LD}, learning rate: {scheduler._last_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncdSFHGWUprR"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "m9u6yVC_Ogh0"
   },
   "outputs": [],
   "source": [
    "# test data\n",
    "test_X = np.load(\"test.npy\", allow_pickle=True)\n",
    "test_dataset = LASDataset(test_X, None, letter2index)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9tPOCDQqTkdp"
   },
   "outputs": [],
   "source": [
    "# test\n",
    "checkpoint = torch.load(f\"model/{model_name}/best_checkpoint\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "prediction = test(model, test_dataloader)\n",
    "with open(\"test_result.csv\", 'w') as f:\n",
    "    f.write(\"id,label\\n\")\n",
    "    for i, pred in enumerate(prediction):\n",
    "        f.write(f\"{i},{pred}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSVQuNdU2qTM"
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c 11785-homework-4-part-2-las-slack -f test_result.csv -m \"second submission\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LockedDropout(nn.Module):\n",
    "    \"\"\" LockedDropout applies the same dropout mask to every time step.\n",
    "\n",
    "    ref: https://github.com/salesforce/awd-lstm-lm/blob/master\n",
    "\n",
    "    Args:\n",
    "        dropout (float): Probability of an element in the dropout mask to be zeroed.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout=0.5):\n",
    "        self.dropout = dropout\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (:class:`torch.FloatTensor` [sequence length, batch size, rnn hidden size]): Input to\n",
    "                apply dropout too.\n",
    "        \"\"\"\n",
    "        if not self.training or not self.p:\n",
    "            return x\n",
    "        x = x.clone()\n",
    "        mask = x.new_empty(1, x.size(1), x.size(2), requires_grad=False).bernoulli_(1 - self.dropout)\n",
    "        mask = mask.div_(1 - self.dropout)\n",
    "        mask = mask.expand_as(x)\n",
    "        return x * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yp3qJiOiXjcB"
   },
   "outputs": [],
   "source": [
    "class MiniEncoder(nn.Module):\n",
    "    '''\n",
    "    Encoder takes the utterances as inputs and returns the key, value and unpacked_x_len.\n",
    "    Key and value are linear projections of the output from pBLSTM network for the laster.\n",
    "    '''\n",
    "    def __init__(self, input_dim, encoder_hidden_dim, key_value_size=128):\n",
    "        super(MiniEncoder, self).__init__()\n",
    "        # The first LSTM at the very bottom\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=encoder_hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "         \n",
    "        # The linear transformation for producing Key and Value for attention\n",
    "        # Since you are using bidirectional LSTM, be careful about the size of hidden dimension\n",
    "        # use the same length for key and value\n",
    "        self.key_network = nn.Linear(encoder_hidden_dim * 2, key_value_size)\n",
    "        self.value_network = nn.Linear(encoder_hidden_dim * 2, key_value_size)\n",
    "\n",
    "\n",
    "    # input: N * seq_len * feature_length (padded, seq_len is the longest sequence length)\n",
    "    # output: N * seq_len * (hidden_dim*2)\n",
    "    def forward(self, x, x_len):\n",
    "        # Pass through the first LSTM at the very bottom\n",
    "        packed_sequence = rnn.pack_padded_sequence(x, x_len.cpu(), enforce_sorted=False, batch_first=True)\n",
    "        packed_out, _ = self.lstm(packed_sequence)\n",
    "        output, out_lengths = rnn.pad_packed_sequence(packed_out, batch_first=True)   # N * seq_len * (hidden*2), with N lengths\n",
    "        \n",
    "        # Unpack the sequence and get the Key and Value for attention\n",
    "        # shape is N * seq_len * key_value_size\n",
    "        key = self.key_network(output)\n",
    "        value = self.value_network(output)\n",
    "\n",
    "        # return key, value, unpacked_x_len\n",
    "        # out_lengths is used for masking in calculating attention\n",
    "        return key, value, out_lengths, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HX31HdnCXgxO"
   },
   "outputs": [],
   "source": [
    "class MiniDecoder(nn.Module):\n",
    "    '''\n",
    "    As mentioned in a previous recitation, each forward call of decoder deals with just one time step.\n",
    "    Thus we use LSTMCell instead of LSTM here.\n",
    "    The output from the second LSTMCell can be used as query for calculating attention.\n",
    "    In place of value that we get from the attention, this can be replace by context we get from the attention.\n",
    "    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance.\n",
    "    '''\n",
    "    def __init__(self, vocab_size, decoder_hidden_dim, embed_dim, key_value_size=128, max_out_len=600, teacher_forcing=0.9, gumbel_noise=0.9):\n",
    "        super(MiniDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=letter2index['<eos>'])\n",
    "        self.lstm1 = nn.LSTMCell(input_size=embed_dim + key_value_size, hidden_size=key_value_size)\n",
    "        # in this implementation, the size of (key = query) = (value = context) = decoder_hidden\n",
    "\n",
    "        self.attention = Attention()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.character_prob = nn.Linear(2 * key_value_size, vocab_size)   # input_sim is key_value_size(context) + hidden_size(hidden_state)\n",
    "        self.character_prob.weight = self.embedding.weight\n",
    "        self.key_value_size = key_value_size\n",
    "\n",
    "        self.max_out_len = max_out_len      # default maximum output sequence length\n",
    "        self.teacher_forcing = teacher_forcing  # teacher forcing probability\n",
    "        self.gumbel_noise = gumbel_noise     # gumbel noise rate\n",
    "\n",
    "    def forward(self, key, value, encoder_len, y=None, mode='train'):\n",
    "        '''\n",
    "        one time step forward\n",
    "        Args:\n",
    "            key :(B, T, key_value_size) - Output of the Encoder Key projection layer\n",
    "            value: (B, T, key_value_size) - Output of the Encoder Value projection layer\n",
    "            y: (B, text_len) - Batch input of text label with text_length, used in teacher forcing\n",
    "            mode: Train or eval mode\n",
    "        Return:\n",
    "            predictions: the character perdiction probability \n",
    "        '''\n",
    "\n",
    "        batch_size, key_seq_max_len, key_value_size = key.shape\n",
    "\n",
    "        # get maximum length of results, or set to default value in testing, then just generate sequences of that length\n",
    "        # get teacher forcing ground truths in training\n",
    "        if mode == 'train' or mode == \"pretrain\":\n",
    "            max_len =  y.shape[1]\n",
    "            label_embeddings = self.embedding(y)\n",
    "        else:\n",
    "            max_len = self.max_out_len\n",
    "\n",
    "        # TODO: Create the attention mask here (outside the for loop rather than inside) to avoid repetition\n",
    "        mask = torch.arange(key.size(1)).unsqueeze(0) >= encoder_len.unsqueeze(1)   # Make use of broadcasting: (1, max_len), (batch_size, 1) -> (batch_size, max_len)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        # output of every time step\n",
    "        predictions = []\n",
    "        prediction = torch.zeros(batch_size, 1).to(device)  # initialize to text output <sos> instead of logits in every batch for first input, compatible with the calculation of char_embed for logits below\n",
    "        hidden_states = [None, None]  # initial hidden states of every layer\n",
    "        \n",
    "        # TODO: Initialize the context. Be careful here\n",
    "        # initialize context to 0 since the first timestep doesn't have context\n",
    "        context = torch.zeros(batch_size, key_value_size).to(device)\n",
    "        \n",
    "        attention = []    # attention score of the first sample in this batch\n",
    "        for i in range(max_len):\n",
    "            # get one-time-step embedding in a batch of shape (batch_size, embed_size)\n",
    "            if mode == 'train' or mode == \"pretrain\":\n",
    "                # TODO: Implement (1) Teacher Forcing and (2) Gumble Noise techniques here\n",
    "                # ...###########################################################################################################################\n",
    "                char_embed = self.embedding(prediction.argmax(dim=-1))\n",
    "            else:\n",
    "                if i>0 and i<20:\n",
    "                    print(f\"i={i} prediction: {prediction.argmax(dim=-1)[0]}\")   # focus on the first sample in the batch\n",
    "                char_embed = self.embedding(prediction.argmax(dim=-1))\n",
    "            \n",
    "            y_context = torch.cat([char_embed, context], dim=1)   # (batch_size, embed_size+key_value_size)\n",
    "            hidden_states[0] = self.lstm1(y_context, hidden_states[0])\n",
    "            output = hidden_states[0][0]    # (batch_size, hidden_size)\n",
    "            if mode == \"test\" and i>0 and i<20:\n",
    "                print(f\"context: {context[0]}\\nattention_score: {attention_score[0]}\")\n",
    "                print(f\"query: {output[0]}\\n\")\n",
    "            \n",
    "            # TODO: Compute attention from the output of the second LSTM Cell\n",
    "            if mode != \"pretrain\":\n",
    "                context, attention_score = self.attention(output, key, value, mask)\n",
    "                attention.append(attention_score[0])    # add attention score of the first sample on current time step\n",
    "            \n",
    "            output_context = torch.cat([output, context], dim=1)\n",
    "            prediction = self.character_prob(output_context)    # (batch_size, vocab_size)\n",
    "            predictions.append(prediction.unsqueeze(1))\n",
    "        if mode == \"pretrain\":\n",
    "            return torch.cat(predictions, dim=1)\n",
    "        else:\n",
    "            return torch.cat(predictions, dim=1), torch.stack(attention, dim=0)    # predictions (batch_size, out_max_len, vocab_size), attention (out_max_len, in_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VVUL-rQObpL7"
   },
   "outputs": [],
   "source": [
    "class MiniLASmodel(nn.Module):\n",
    "    '''\n",
    "    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder.\n",
    "    This is simply a wrapper \"model\" for your encoder and decoder.\n",
    "    '''\n",
    "    def __init__(self, input_dim, vocab_size, encoder_hidden_dim, decoder_hidden_dim, embed_dim, key_value_size=128):\n",
    "        super(MiniLASmodel,self).__init__()\n",
    "        self.encoder = MiniEncoder(input_dim, encoder_hidden_dim, key_value_size=key_value_size)\n",
    "        self.decoder = MiniDecoder(vocab_size, decoder_hidden_dim, embed_dim, key_value_size=key_value_size)\n",
    "\n",
    "    def forward(self, x, x_len, y=None, mode='train'):\n",
    "        key, value, encoder_len, out = self.encoder(x, x_len)\n",
    "        \n",
    "        if mode == \"pretrain\":\n",
    "            key *= 0\n",
    "            value *= 0\n",
    "            out *= 0\n",
    "        if mode == \"test\":\n",
    "            print(f\"out\\n{out}{out.shape}\\nkey\\n{key}{key.shape}\\nvalue\\n{value}{value.shape}\\n\")\n",
    "            print(f\"key_weight\\n{self.encoder.key_network.weight}\\n\")\n",
    "            print(f\"key_bias\\n{self.encoder.key_network.bias}\\n\")\n",
    "            print(f\"value_weight\\n{self.encoder.value_network.weight}\\n\")\n",
    "            print(f\"value_bias\\n{self.encoder.value_network.bias}\\n\")\n",
    "        if mode == \"pretrain\":\n",
    "            predictions = self.decoder(key, value, encoder_len, y=y, mode=mode)\n",
    "            attention = None\n",
    "        else:\n",
    "            predictions, attention = self.decoder(key, value, encoder_len, y=y, mode=mode)\n",
    "        return predictions, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PD2i6ahxcr7b"
   },
   "outputs": [],
   "source": [
    "# train one epoch, return the average training loss and attention\n",
    "def train_epoch_mini(model, train_loader, criterion, optimizer, mode):\n",
    "    training_loss = 0\n",
    "    model.train()\n",
    "    # 0) Iterate through your data loader\n",
    "    for batch_num, (padded_inputs, padded_labels, input_lengths, label_lengths) in tqdm(enumerate(train_loader)):\n",
    "        # 1) Set the inputs to the device.\n",
    "        padded_inputs = padded_inputs.to(device)\n",
    "        padded_labels = padded_labels.to(device)\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        label_lengths = label_lengths.to(device)\n",
    "        # 2) Pass your inputs, and length of speech into the model.\n",
    "        # the input to the decoder should be <sos>+sequence and output sequence+<eos>, while padded_labels=<sos>+sequence+<eos>\n",
    "        # so remove the last element of each label sequence to fix shape\n",
    "        predictions, _ = model(padded_inputs, input_lengths, padded_labels[:,:-1], mode=mode)\n",
    "        # 3) Generate a mask based on the lengths of the text\n",
    "        #    Ensure the mask is on the device and is the correct shape.\n",
    "        # use length-1 to mask out predictions out of <eos>\n",
    "        mask_loss = torch.arange(padded_labels.size(1)-1).unsqueeze(0).to(device) >= (label_lengths-1).unsqueeze(1)\n",
    "        # 4. Calculate the loss and mask it to remove the padding part\n",
    "        batch_size, max_len, vocab_size = predictions.shape\n",
    "\n",
    "        loss = criterion(predictions.reshape(-1, vocab_size), padded_labels[:,1:].reshape(-1)) # remove <sos> in labels to calculate loss\n",
    "        loss.masked_fill_(mask_loss.reshape(-1).to(device), 0)\n",
    "        loss = loss.sum()   # add up losses of all time steps, then divide by batch size\n",
    "        # loss = loss / ((mask_loss == False).sum())\n",
    "        loss /= batch_size\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "        # 5. Backward on the masked loss\n",
    "        loss.backward()\n",
    "        # 6. Optional: Use torch.nn.utils.clip_grad_norm(model.parameters(), 2) to clip the gradient\n",
    "        # 7. Take a step with your optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "    training_loss /= len(train_loader)\n",
    "    # 8. print the statistic (loss, edit distance and etc.) for analysis\n",
    "    return training_loss\n",
    "        \n",
    "\n",
    "# validation of classification task, return the average loss and LD\n",
    "def evaluate_mini(model, val_loader, criterion):\n",
    "    val_loss = 0\n",
    "    val_LD = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (padded_inputs, padded_labels, input_lengths, label_lengths) in enumerate(val_loader):\n",
    "            padded_inputs = padded_inputs.to(device)\n",
    "            padded_labels = padded_labels.to(device)\n",
    "            input_lengths = input_lengths.to(device)\n",
    "            label_lengths = label_lengths.to(device)\n",
    "\n",
    "            predictions, attention_score = model(padded_inputs, input_lengths, padded_labels[:,:-1], mode=\"test\")\n",
    "            # calculate masked loss\n",
    "            # prediction will be of shape (batch_size, 600, vocab_size) since it's in test mode, need to truncate\n",
    "            predictions = predictions[:,:padded_labels.shape[1]-1]\n",
    "\n",
    "            plot_attention(attention_score[:padded_labels.shape[1]-1].cpu().detach().numpy())\n",
    "            \n",
    "            mask_loss = torch.arange(padded_labels.size(1)-1).unsqueeze(0).to(device) >= (label_lengths-1).unsqueeze(1)\n",
    "            batch_size, max_len, vocab_size = predictions.shape\n",
    "            \n",
    "            loss = criterion(predictions.reshape(-1, vocab_size), padded_labels[:,1:].reshape(-1)) # remove <sos> in labels to calculate loss\n",
    "            loss.masked_fill_(mask_loss.reshape(-1).to(device), 0)\n",
    "            loss = loss.sum()   # add up losses of all time steps, then divide by batch size\n",
    "            # loss = loss / ((mask_loss == False).sum())\n",
    "            loss /= batch_size\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # simple greedy search decoding\n",
    "            predicted_indices = torch.argmax(predictions, dim=2)\n",
    "            print(predicted_indices)\n",
    "            # convert result back to text, and compute LD\n",
    "            batch_LD = 0\n",
    "            for i in range(batch_size):\n",
    "                letter_seq = transform_index_to_letter(predicted_indices[i], index2letter)\n",
    "                letter_label_seq = transform_index_to_letter(padded_labels[i][1:], index2letter)\n",
    "                val_LD += distance(letter_seq, letter_label_seq)\n",
    "            batch_LD /= batch_size\n",
    "            val_LD += batch_LD\n",
    "    val_loss /= len(val_loader)\n",
    "    val_LD /= len(val_loader)\n",
    "    return val_loss, val_LD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 12097,
     "status": "ok",
     "timestamp": 1620577456738,
     "user": {
      "displayName": "Ruiyang Jin",
      "photoUrl": "",
      "userId": "11822135231472973432"
     },
     "user_tz": -480
    },
    "id": "bUcyVMHEkcp4"
   },
   "outputs": [],
   "source": [
    "# try out on simple dataset\n",
    "feature_length = 40\n",
    "learningRate = 1e-3\n",
    "weightDecay = 1e-6\n",
    "vocab_size = len(index2letter)\n",
    "listener_hidden_size = 256\n",
    "speller_hidden_size = 512\n",
    "key_value_size = 128\n",
    "embed_size = 256\n",
    "model = MiniLASmodel(feature_length, vocab_size, listener_hidden_size, speller_hidden_size, embed_size, key_value_size)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = optim.Adam(model.parameters(), lr=learningRate, weight_decay=weightDecay)\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, gamma=0.5, step_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJ-MfMmVkUPq"
   },
   "outputs": [],
   "source": [
    "# pretrain model\n",
    "epoch_pretrain = 10\n",
    "for epoch in range(epoch_pretrain):\n",
    "    training_loss = train_epoch_mini(model, train_dataloader, criterion, optimizer, mode=\"pretrain\")\n",
    "    val_loss, val_LD = evaluate_mini(model, val_dataloader, criterion)\n",
    "    # scheduler.step(val_LD)\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch: {epoch}, training loss: {training_loss}, validation loss: {val_loss}, validation Levenshtein Distance: {val_LD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1Njn-tXQibZbWUIbzf2A6p-0TMc9ZrBkV"
    },
    "executionInfo": {
     "elapsed": 176127,
     "status": "ok",
     "timestamp": 1620057701126,
     "user": {
      "displayName": "Ruiyang Jin",
      "photoUrl": "",
      "userId": "11822135231472973432"
     },
     "user_tz": -480
    },
    "id": "KC-GwkS-bd2E",
    "outputId": "a77ab0f9-68a5-4f25-9635-ab34896ac01f"
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "epoch_num = 10\n",
    "for epoch in range(epoch_num):\n",
    "    training_loss = train_epoch_mini(model, train_dataloader, criterion, optimizer, mode=\"train\")\n",
    "    val_loss, val_LD = evaluate_mini(model, val_dataloader, criterion)\n",
    "    # scheduler.step(val_LD)\n",
    "    # scheduler.step()\n",
    "    print(f\"Epoch: {epoch}, training loss: {training_loss}, validation loss: {val_loss}, validation Levenshtein Distance: {val_LD}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "p2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Environment (conda_11785)",
   "language": "python",
   "name": "conda_11785"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
